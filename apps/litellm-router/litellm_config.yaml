# LiteLLM Configuration for Cloudflare Router
# This configuration defines the available models and their routing
# Based on https://github.com/memorysaver/dotfiles/blob/main/litellm/config.yaml

model_list:
  # Anthropic Models (Wildcard pattern for all Claude models)
  - model_name: anthropic/*
    litellm_params:
      model: anthropic/*
      api_key: os.environ/ANTHROPIC_API_KEY
      max_tokens: 65536
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8

  # OpenRouter Models (Access to 100+ models via single API)
  - model_name: openrouter/*
    litellm_params:
      model: openrouter/*
      api_key: os.environ/OPENROUTER_API_KEY
      max_tokens: 65536
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8

  # Groq Models (Fast inference)
  - model_name: groq/*
    litellm_params:
      model: groq/*
      api_key: os.environ/GROQ_API_KEY
      max_tokens: 65536
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8

  # Cerebras Models (High-performance inference)
  - model_name: cerebras/*
    litellm_params:
      model: cerebras/*
      api_key: os.environ/CEREBRAS_API_KEY
      max_tokens: 65536
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8

  # OpenAI Models (Optional - can use OpenRouter instead)
  - model_name: openai/*
    litellm_params:
      model: openai/*
      api_key: os.environ/OPENAI_API_KEY
      max_tokens: 65536
      repetition_penalty: 1.05
      temperature: 0.7
      top_k: 20
      top_p: 0.8

# Router settings for load balancing and reliability
router_settings:
  routing_strategy: 'least-busy'
  enable_loadbalancing: true
  retry_policy:
    max_retries: 3
    timeout: 30
    exponential_backoff: true

# General LiteLLM settings
general_settings:
  # Master key for authentication (can be overridden by environment)
  master_key: os.environ/LITELLM_MASTER_KEY

  # Enable detailed logging for debugging
  set_verbose: true

  # Drop unsupported parameters to prevent errors
  drop_params: true

  # Enable request/response caching for better performance
  cache: true
  cache_type: 'redis' # Will fallback to in-memory if Redis not available

# LiteLLM specific settings
litellm_settings:
  # Drop parameters that aren't supported by the target model
  drop_params: true

  # Set detailed logging
  set_verbose: true

  # Enable streaming for better user experience
  stream: true
