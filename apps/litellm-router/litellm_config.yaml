# LiteLLM Configuration for Cloudflare Router
# This configuration defines the available models and their routing

model_list:
  # OpenAI Models
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY

  # Anthropic Models
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

# Router settings for load balancing and reliability
router_settings:
  routing_strategy: 'least-busy'
  enable_loadbalancing: true
  retry_policy:
    max_retries: 3
    timeout: 30
    exponential_backoff: true

# General LiteLLM settings
general_settings:
  # Master key for authentication (can be overridden by environment)
  master_key: os.environ/LITELLM_MASTER_KEY

  # Enable detailed logging for debugging
  set_verbose: true

  # Drop unsupported parameters to prevent errors
  drop_params: true

  # Enable request/response caching for better performance
  cache: true
  cache_type: 'redis' # Will fallback to in-memory if Redis not available

# LiteLLM specific settings
litellm_settings:
  # Drop parameters that aren't supported by the target model
  drop_params: true

  # Set detailed logging
  set_verbose: true

  # Enable streaming for better user experience
  stream: true
